{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bd49ff49425165a146a9db9a89408468daf1e0b5"
   },
   "source": [
    "# Classifying Fraudulent and Valid Transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fd376936ce69ca28360a80814a602b9191ac9dca"
   },
   "source": [
    "###  Loading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "5d7381e5a304f419c0ea2119d90e2d793f697766"
   },
   "outputs": [],
   "source": [
    "# loading needed methods\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "from random import seed,sample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score, roc_curve, auc,\\\n",
    "precision_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "71479e82537154fac60290ccb882f2ec1d7c778b"
   },
   "outputs": [],
   "source": [
    "# loading data\n",
    "\n",
    "data = pd.read_csv('PS_20174392719_1491204439457_log.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "faaa730c3625768c574b487b99e74ae4e6d26a65"
   },
   "source": [
    "### Looking at Account Types "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dbc73f8304a9ca9d9bd76d78e5ebc77672a2f266"
   },
   "source": [
    " a feature \"type1\" is created which is a categorical variable with levels \"CC\" (Customer to Customer), \"CM\" (Customer to Merchant), \"MC\" (Merchant to Customer), \"MM\" (Merchant to Merchant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6010843c4f59c40e7e4d11d7c3630625c0b57dea"
   },
   "outputs": [],
   "source": [
    "# adding feature type1\n",
    "data_new = data.copy() # creating copy of dataset in case I need original dataset\n",
    "data_new[\"type1\"] = np.nan # initializing feature column\n",
    "\n",
    "# filling feature column\n",
    "data_new.loc[data.nameOrig.str.contains('C') & data.nameDest.str.contains('C'),\"type1\"] = \"CC\" \n",
    "data_new.loc[data.nameOrig.str.contains('C') & data.nameDest.str.contains('M'),\"type1\"] = \"CM\"\n",
    "data_new.loc[data.nameOrig.str.contains('M') & data.nameDest.str.contains('C'),\"type1\"] = \"MC\"\n",
    "data_new.loc[data.nameOrig.str.contains('M') & data.nameDest.str.contains('M'),\"type1\"] = \"MM\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0d11336305c5897c1569854023135f5e988b9950"
   },
   "outputs": [],
   "source": [
    "# Subsetting data into observations with fraud and valid transactions:\n",
    "fraud = data_new[data_new[\"isFraud\"] == 1]\n",
    "valid = data_new[data_new[\"isFraud\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c4a54ce2d3ed6115b7af7c6fe9c52e40b32dcd6"
   },
   "outputs": [],
   "source": [
    "# seeing the counts of transactions by type1 (CC,CM,MC,MM)\n",
    "print(\"Fraud transactions by type1: \\n\",fraud.type1.value_counts())\n",
    "print(\"\\n Valid transactions by type1: \\n\",valid.type1.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b3437a7f2ea69f9ea262e6a4e88733f567bb260b"
   },
   "source": [
    "#### Conclusion: \n",
    "\n",
    "From the dataset, it seems that fraud transactions only occur when the transaction type1 is CC (Customer to Customer)\n",
    "We can assume that transaction only occurs when transaction type is CC. This also means that the datasets fraud and valid don't need to be subsetted. However, since all relevant observations have type1 = \"CC\", the type1 column is no longer necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "def72f3a4cf035009e672315960a27a56dee6781"
   },
   "outputs": [],
   "source": [
    "# getting rid of type1 column.\n",
    "\n",
    "fraud = fraud.drop('type1', 1)\n",
    "valid = valid.drop('type1',1)\n",
    "data_new = data_new.drop('type1',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6e44c0b3b845746cb7c3376694a1938011dd2f5e"
   },
   "source": [
    "###  Looking at Transaction Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "967f70be1104b934e5d9ba05d6ee3bbbdd1c0985"
   },
   "outputs": [],
   "source": [
    "# seeing the counts of transactions by type\n",
    "print(\"Fraud transactions by type: \\n\",fraud.type.value_counts())\n",
    "print(\"\\n Valid transactions by type: \\n\",valid.type.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4622ec0cf0fd9c663ede0983210563ddea1e30ed"
   },
   "source": [
    "#### Conclusion: \n",
    "\n",
    "From the dataset, it seems that fraud transactions only occur when the transaction type is CASH_OUT or TRANSFER.\n",
    "Here we will assume that transaction only occur when transaction type is either CASH_OUT or TRANSFER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a45da65b70abfe8fb0f60a5bf76103bd20d9f415"
   },
   "outputs": [],
   "source": [
    "# Subsetting data according to the conclusion above\n",
    "# I don't have to subset for the fraud dataset because all of their transaction types are either TRANSFER or CASH_OUT\n",
    "\n",
    "valid = valid[(valid[\"type\"] == \"CASH_OUT\")| (valid[\"type\"] == \"TRANSFER\")]\n",
    "data_new = data_new[(data_new[\"type\"] == \"CASH_OUT\") | (data_new[\"type\"] == \"TRANSFER\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "afd8203ae6365bda92ba15e83dec1bc362658a43"
   },
   "source": [
    "### Looking balances before and after the transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "16cffdae719b47c107766e5c2da0291016cb099d"
   },
   "outputs": [],
   "source": [
    "# adding features errorBalanceOrg, errorBalanceDest\n",
    "data_new[\"errorBalanceOrg\"] = data_new.newbalanceOrig + data_new.amount - data_new.oldbalanceOrg\n",
    "data_new[\"errorBalanceDest\"] = data_new.oldbalanceDest + data_new.amount - data_new.newbalanceDest\n",
    "\n",
    "# Subsetting data into observations with fraud and valid transactions:\n",
    "fraud = data_new[data_new[\"isFraud\"] == 1]\n",
    "valid = data_new[data_new[\"isFraud\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c6de711f5589f9a13b8ad6b6bb7bc49ed409315d"
   },
   "outputs": [],
   "source": [
    "errors = [\"errorBalanceOrg\", \"errorBalanceDest\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8b1419b7c9b32585b4c7c0ebf26f0937bba59264"
   },
   "source": [
    "### Another Look at Transaction Types and Account Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "deec337aca21953afaddf2b4e4ee5ff27d0d8348"
   },
   "source": [
    "According to the overview of the dataset on kaggle:\n",
    "\n",
    "\n",
    "*This is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control or customers accounts and try to empty the funds by transferring to another account and then cashing out of the system.*\n",
    "\n",
    "Let's see if this is reflected in the fraud dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc09a8621245efe3202d401e6e2e97913706f9d9"
   },
   "outputs": [],
   "source": [
    "print(\"Fraud transactions by type: \\n\",fraud.type.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4c70b925251374537d45a3696afc9a511bef6fee"
   },
   "source": [
    "Clearly, fraudulent transactions exclusively involved cashouts and transfers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a31a17b27e1d232e0a4cb35079c557b1a6f7f118"
   },
   "source": [
    "However, in this sample the account that the money to transferred to tends to not be the account used to make the cashout. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "90754ed1b066d6b045703107e0ad68e9d3925c7e"
   },
   "outputs": [],
   "source": [
    "# separating transfers and cashouts for fraud accounts\n",
    "\n",
    "fraud_transfer = fraud[fraud[\"type\"] == \"TRANSFER\"]\n",
    "fraud_cashout = fraud[fraud[\"type\"] == \"CASH_OUT\"]\n",
    "\n",
    "# checking if the recipient account of a fraudulent transfer was used as a sending account for cashing out \n",
    "fraud_transfer.nameDest.isin(fraud_cashout.nameOrig).any()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7661c37d931634c4cbbb88207cfaeb75562e0719"
   },
   "source": [
    "### Conclusion:\n",
    "\n",
    "Thus in this dataset, for fraudulent transactions, the account that received funds during a transfer was not used at all for cashing out.\n",
    "\n",
    "If that is the case, there seems to be no use for nameOrig or nameDest since there seems to be no restrictions on which accounts cashout from fraudulent transactions.\n",
    "\n",
    "Thus, omitting the nameOrig and nameDest columns from analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2ba42cadcc6cdacfb9dc1381b3e0a6c303cf6e77"
   },
   "outputs": [],
   "source": [
    "# getting rid of nameOrig and nameDest column.\n",
    "names = [\"nameOrig\",\"nameDest\"]\n",
    "fraud = fraud.drop(names, 1)\n",
    "valid = valid.drop(names,1)\n",
    "data_new = data_new.drop(names,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "317485c6424fcd06968f32a1d3c2ed6792f432a4"
   },
   "source": [
    "### Looking at Flagged Transactions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "23c98e4ca55081610acb3696c6d9258e3347b041"
   },
   "source": [
    "From the overview, the variable isFlaggedFraud is described as transactions that were flagged as fraud.\n",
    "\n",
    "To be flagged as fraud, the transaction would have to be fraudulent and involve a transfer of more than 200, 000 units in a specified currency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c651cfe9a2b6eeeaa6d5e1b2c25f886218b02dd0"
   },
   "outputs": [],
   "source": [
    "# how many observations were flagged as Fraud?\n",
    "flagged = data_new[data_new[\"isFlaggedFraud\"] == 1]\n",
    "flagged_correctly = sum(flagged[\"isFraud\"] == 1)\n",
    "flagged_wrongly = len(flagged) - flagged_correctly\n",
    "total = flagged_correctly + flagged_wrongly\n",
    "print(flagged_correctly,\" observations were flagged correctly and \", flagged_wrongly, \\\n",
    "      \" observations were flagged wrongly for a total of \", total, \" flagged observations.\")\n",
    "\n",
    "# how many observations where the transaction is fraudulent, the transaction is a transfer and the amount is greater \n",
    "# than 200, 000 are in the dataset\n",
    "should_be_flagged = fraud[(fraud[\"amount\"] > 200000) & (fraud[\"type\"] == \"TRANSFER\")]\n",
    "print(\"number of observations that should be flagged: \",len(should_be_flagged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fcad7a9e8fc5422cdda49c755c3e9af86644e15e"
   },
   "source": [
    "### Conclusion: \n",
    "\n",
    "In a modified dataset with more than 2 million observations, a variable that brings attention to only 16 observations is insignificant.\n",
    "\n",
    "Furthermore, the number of transactions that should have been flagged far exceeds the number of observations that were actually flagged.\n",
    "\n",
    "In addition, as we are trying to develop a new fraud detection screen that does not depend on a pre-existing fraud detection scheme.\n",
    "\n",
    "For that reason, we are omitting the isFlaggedFraud column from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9d5941c2138c57991ceae60a9e63bdfc8540d587"
   },
   "outputs": [],
   "source": [
    "# dropping isFlaggedFraud column from the fraud,valid, and new_data datasets\n",
    "\n",
    "fraud = fraud.drop(\"isFlaggedFraud\",1)\n",
    "valid = valid.drop(\"isFlaggedFraud\",1)\n",
    "data_new = data_new.drop(\"isFlaggedFraud\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "455b8e8ab3b1ead3de4b8d01ca0e52cb49d56d1e"
   },
   "outputs": [],
   "source": [
    "dataset1 = data_new.copy()\n",
    "\n",
    "\n",
    "# adding feature HourOfDay to Dataset1 \n",
    "dataset1[\"HourOfDay\"] = np.nan # initializing feature column\n",
    "dataset1.HourOfDay = data_new.step % 24\n",
    "\n",
    "\n",
    "print(\"Head of dataset1: \\n\", pd.DataFrame.head(dataset1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "061fbd106c2033815f4fd26804d0095e4168b8cb"
   },
   "source": [
    "### Looking at Amounts Moved in Transactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "68cd168a839046714b8429831eb6ce3e27cd7343"
   },
   "outputs": [],
   "source": [
    "# Seeing summary statistics of the data\n",
    "\n",
    "print(\"Summary statistics on the amounts moved in fraudulent transactions: \\n\",pd.DataFrame.describe(fraud.amount),\"\\n\")\n",
    "print(\"Summary statistics on the amounts moved in valid transactions: \\n\", pd.DataFrame.describe(valid.amount),\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e56ad2b677c9961e1b63c945e4031e9d6f05d024"
   },
   "source": [
    "It seems that during fraudulent transactions, the amount moved is capped at 10 million currency units.\n",
    "\n",
    "Whereas for valid transactions, the amount moved is capped at about 92.4 million currency units.\n",
    "\n",
    "when plotting time-steps against amount moved we get this plot...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8a27b5f0a6849e6342254f843c843c85c5cb09cf"
   },
   "outputs": [],
   "source": [
    "# plotting overlayed step vs amount scatter plots\n",
    "\n",
    "alpha = 0.3\n",
    "fig,ax = plt.subplots()\n",
    "valid.plot.scatter(x=\"step\",y=\"amount\",color=\"green\",alpha=alpha,ax=ax,label=\"Valid Transactions\")\n",
    "fraud.plot.scatter(x=\"step\",y=\"amount\",color=\"red\",alpha=alpha,ax=ax, label=\"Fraudulent Transactions\")\n",
    "\n",
    "plt.title(\"1 hour timestep vs amount\")\n",
    "plt.xlabel(\"1 hour time-step\")\n",
    "plt.ylabel(\"amount moved in transaction\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "# plotting a horizontal line to show where valid transactions behave very differently from fraud transactions\n",
    "\n",
    "plt.axhline(y=10000000)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Proportion of transactions where the amount moved is greater than 10 million: \", \\\n",
    "      len(data_new[data_new.amount > 10000000])/len(data_new))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "52211c3fc4eea57255744989f6ce4faa3ec42f3b"
   },
   "source": [
    "### Conclusion:\n",
    "\n",
    "Only valid transaction involved amounts larger than 10,000,000, however these transactions make up less than 0.01% of the relevant data.\n",
    "\n",
    "When the amounts moved is less than 10,000,000 there doesn't seem to be a large difference fraudulent and valid transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "08bc603d201171aeaaa1c1c9fd9a19cd7330093f"
   },
   "outputs": [],
   "source": [
    "# finalizing dataset\n",
    "dataset = dataset1.copy() # unchanged dataset1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "13ed100736440c745b4174188c577fab4aad9bc4"
   },
   "source": [
    "## Pre-processing Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0dee8617423d3aec4f4a3e869664e0def136ff08"
   },
   "source": [
    "### Handling Categorical Variables\n",
    "\n",
    "\n",
    "If an observation is part of a particular category (e.g. the transaction type is CASH_OUT), the indicator variable associated with the category would be 1. If it isn't part of a particular category, then the indicator variable associated with that category would be 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "92f5bfb1f6dd1d8291b1112d2df097413cd03378"
   },
   "outputs": [],
   "source": [
    "# getting one-hot encoding of the 'type' variable\n",
    "\n",
    "dataset = pd.get_dummies(dataset,prefix=['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "82a78ef5ec83d05597051dd4a551dec91f93cd28"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.head(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dcf78dd0b164fef2e492248dbedff35f4d28845b"
   },
   "source": [
    "## Splitting and Standardizing Data.\n",
    "\n",
    "Similarly, many, if not all, machine learning algorithms perform better when the data is standardized/normalized (when all values are between 0 and 1 inclusive).\n",
    "\n",
    "We will do this to standardize the data without standardizing the target variable isFraud.\n",
    "\n",
    "Additionally, we will also split the data up into training sets and testing sets. A common split is to separate 80% of the data as the training set and the rest as the testing set. However we will rely on the \"default\" split which is 75% of the data is used as the training set, 25% is used as the testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0018683e4c757a67a7b3fde1be8d9a533e992233"
   },
   "outputs": [],
   "source": [
    "# Setting random_state and seed so that the training/testing splits and model results are reproducible\n",
    "RandomState = 42\n",
    "seed(21)\n",
    "\n",
    "\n",
    "# 42 is used often due to Hitchhiker's Guide to the Galaxy, I will use a number that a far smaller group may understand.\n",
    "# Not that the actual number doesn't matter and is only used to make sure results are reproducible.\n",
    "# creating training and testing sets\n",
    "X = dataset.drop(\"isFraud\",1)\n",
    "y = dataset.isFraud\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "# Normalizing data so that all variables follow the same scale (0 to 1)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Now apply the transformations to the data:\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9cb20c705c396b934f165e81b0a732ad34bb50fb"
   },
   "source": [
    "##  Model Selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "83a943645260f03b38334868a505c86bd4ba60b3"
   },
   "source": [
    "###  Model 1: Random Forest.\n",
    "\n",
    "A random forest is an algorithm that generates several decisions trees and pools the results of each tree to make a more robust prediction. \n",
    "\n",
    "Another great thing about Random Forest is that is weights can be assigned to each class to reduce the bias of the model towards the majority class, in this case valid transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "678d5e9f3078534be720e2d2b19d50815bfafe5b"
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "parametersRF = {'n_estimators':15,'oob_score':True,'class_weight': \"balanced\",'n_jobs':-1,\\\n",
    "                 'random_state':RandomState}\n",
    "RF = RandomForestClassifier(**parametersRF)\n",
    "fitted_vals = RF.fit(X_train, y_train)\n",
    " \n",
    "# Predict on testing set\n",
    "predictionsRF = RF.predict(X_test)\n",
    " \n",
    "     \n",
    "# Evaluating model\n",
    "CM_RF = confusion_matrix(y_test,predictionsRF)\n",
    "CR_RF = classification_report(y_test,predictionsRF)\n",
    "fprRF, recallRF, thresholdsRF = roc_curve(y_test, predictionsRF)\n",
    "AUC_RF = auc(fprRF, recallRF)\n",
    "accuracyRF = accuracy_score(y_test,predictionsRF)\n",
    "resultsRF = {\"Confusion Matrix\":CM_RF,\"Classification Report\":CR_RF,\"Area Under Curve\":AUC_RF,\"Accuracy score\":accuracyRF}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store accuracyRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "598653e9bd99248e6cc2cc068443fdc62b234ee9"
   },
   "outputs": [],
   "source": [
    "# showing results from Random Forest\n",
    "\n",
    "for measure in resultsRF:\n",
    "    print(measure,\": \\n\",resultsRF[measure])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ab838125e069d1dc87be0551f33baee37c8a788b"
   },
   "source": [
    "###  Model 2: e**X**treme **G**radient **B**oosting Trees (or XGB trees for short)\n",
    "\n",
    "\n",
    "This algorithm is well known for being used in imbalanced datasets. Similar to Random Forests, the algorithm generates several decision trees and pooling the results. \n",
    "\n",
    "However,instead of generating multiple full blown decision trees in parallel and pooling the results, it generates multiple trees formed by weak learners sequentially and then it pools the results .\n",
    "\n",
    "As with Random Forests, weights are set such that the model is less biased towards the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "69142693de84287f4d633a3c57ab6ad730eea378"
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "weights = (y == 0).sum() / (1.0 * (y == 1).sum()) # for unbalanced datasets, these weights are recommended\n",
    "parametersXGB = {'max_depth':3,'scale_pos_weight': weights,'n_jobs':-1,\\\n",
    "                 'random_state':RandomState,'learning_rate':0.1}\n",
    "XGB = XGBClassifier(**parametersXGB)\n",
    "    \n",
    "fitted_vals = XGB.fit(X_train, y_train)\n",
    " \n",
    "# Predict on testing set\n",
    "predictionsXGB = XGB.predict(X_test)\n",
    " \n",
    "     \n",
    "# Evaluating model\n",
    "CM_XGB = confusion_matrix(y_test,predictionsXGB)\n",
    "CR_XGB = classification_report(y_test,predictionsXGB)\n",
    "fprXGB, recallXGB, thresholds_XGB = roc_curve(y_test, predictionsXGB)\n",
    "AUC_XGB = auc(fprXGB, recallXGB)\n",
    "accuracyXGB = accuracy_score(y_test,predictionsRF)\n",
    "resultsXGB = {\"Confusion Matrix\":CM_XGB,\"Classification Report\":CR_XGB,\"Area Under Curve\":AUC_XGB,\"Accuracy score\":accuracyXGB}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracyXGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store accuracyXGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5e260b5509a1af6ad4e3f07e503ab84d8198c4d9"
   },
   "outputs": [],
   "source": [
    "# showing results from Extreme Gradient Boosting\n",
    "for measure in resultsXGB:\n",
    "    print(measure,\": \\n\",resultsXGB[measure],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf01 = CM_RF[0,1]\n",
    "rf10 = CM_RF[1,0]\n",
    "\n",
    "xgb01 = CM_XGB[0,1]\n",
    "xgb10 = CM_XGB[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store rf01\n",
    "%store rf10\n",
    "%store xgb01\n",
    "%store xgb10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
